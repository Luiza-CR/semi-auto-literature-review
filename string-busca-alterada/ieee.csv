"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Analyzing Previous Human-Robot Interaction Implementation in Agriculture: What Can We Learn from the Past?","A. Elias; M. J. Galvez Trigo; C. Camacho-Villa","School of Computer Science, University of Lincoln, Lincoln, United Kingdom; School of Computer Science & Informatics, Cardiff University, Cardiff, United Kingdom; School of Agri-Food Tech & Manufacturing, University of Lincoln, Lincoln, United Kingdom",2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI),"30 Apr 2025","2025","","","123","131","With the recent shift from conventional industrial robots to more collaborative Human-Robot Interaction (HRI) robots11In this paper, the term ‘HRI robot’ is used to underscore the significance of the interaction between robots and humans, rather than focusing on the type of robot involved (e.g., cobot, social robot) or the nature of the interaction (e.g., instruction-based, social interaction-based), when we refer to HRI robots we refer to any type of robot a person/s may interact or engage directly with during a task. within industries such as the agriculture sector, it has become essential to understand the challenges associated with the adoption of these robots to ensure a smooth integration with minimal resistance. As with all new technologies, there is often push-back when changing approaches and initiating new pathways within company operations, which can cause hesitation and even halt the adoption process. This paper draws from interviews with agricultural companies that have previously attempted to implement robots requiring direct human interaction, focusing on individuals within those companies who had decision-making capabilities during the implementation process. From these interviews, a set of action principles has been developed based on transferable knowledge found within the participating companies. The main results of this user study highlight that previous implementation attempts, whether positive or negative, influence future adoption. The study also identifies the multitude of barriers surrounding the agricultural sector's adoption of these technologies and suggests potential actions for companies to take to minimize the issues associated with implementing HRI robots. By identifying common successes and failures and contextualizing them for other companies to follow, this study aims to utilize lessons learned from past implementation attempts to shorten the learning curve and reduce hesitation in adopting HRI robots within the agricultural sector.","","979-8-3503-7893-1","10.1109/HRI61500.2025.10974051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10974051","HRI;pHRI;Interaction;Agriculture;Adoption;Integration","Service robots;Social robots;Human-robot interaction;Focusing;Companies;Industrial robots;Agriculture;Stakeholders;Interviews;Robots","","2","","35","IEEE","30 Apr 2025","","","IEEE","IEEE Conferences"
"3 Role of 6G, IoT with Integration of AI and ML and Security in Agriculture","",,"6G Connectivity-Systems, Technologies, and Applications: Digitalization of New Technologies, 6G and Evolution","","2024","","","43","68","This book covers need for 6G connectivity arising from the pursuit of higher data speeds, ultra-low latency, massive IoT connectivity, enhanced spectral efficiency, and the facilitation of new and transformative applications. By addressing these drivers and expectations, 6G aims to revolutionize wireless communication, opening up a realm of possibilities for industries, societies, and individuals. Technological improvements and evolutions are required beyond fifth-generation (5G) networks for wireless communications as well as in the industry where the involvement of collaborative robots (COBOT) will satisfy the personal needs of human beings as and when required leading to human–machine interactions. A considerable amount of effort has been devoted, both in industry and academia, towards the performance modelling, evaluation and prediction of convergent multi-service heterogeneous, future-generation networks such as 6G. Technical topics discussed in the book include: • Network security and attacks • 6G applications and Industry 5.0 • Human centric interface • Green computing in wireless cellular networks • Next generation networks (IOT, Cloud Computing, Big Data, etc.).","","9788770228701","","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=10440243.pdf&bkn=10440194&pdfType=chapter","","","","","","","","19 Feb 2024","","","River Publishers","River eBook Chapters"
"Validating Point Cloud Registration for Precision Agriculture Using AprilTags","J. Karty; B. Hament","Engineering Department, Elon University, Elon, NC, USA; Engineering Department, Elon University, Elon, NC, USA",2025 Systems and Information Engineering Design Symposium (SIEDS),"6 Jun 2025","2025","","","297","301","3D data allows for better tracking of the health of plants, allowing for optimal care for each plant. This reduces water consumption, decreases reliance on pesticides, and increases crop yield. One major barrier to collecting 3D plant data is registration of point clouds collected from different perspectives. Unlike traditional man-made targets, such as buildings and consumer products, organic targets have high variability in shape and structure. Traditional methods of point cloud registration struggle with these highly variable features. To overcome this, a method of data collection involving fixed artificial markers is presented. A D435i depth camera is attached to the end of a my Cobot 320 M5 robot arm, and a frame of AprilTags is built around the scaffolding the plants grow on. As the plants are scanned, the camera position and orientation is determined by viewing the AprilTags. This information is used to register the point clouds. The localization of this setup is measured to be less than 0.7 cm by placing the depth camera in two positions that are fixed relative to each other, then subtracting the results. The registration error is measured by comparing a known object across scans from different positions. The average root mean square error of the registration is 2.16 mm. This system is then validated by successfully scanning and registering a pink hyacinth and a green pepper sprout.","2994-3531","979-8-3315-3575-9","10.1109/SIEDS65500.2025.11021186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11021186","","Point cloud compression;Location awareness;Accuracy;Target tracking;Shape;Robot vision systems;Position measurement;Three-dimensional printing;Cameras;Root mean square","","","","15","IEEE","6 Jun 2025","","","IEEE","IEEE Conferences"
"Compliant Plant Exploration for Agricultural Procedures With a Collaborative Robot","M. Polic; M. Car; F. Petric; M. Orsag","Faculty of Electrical and Computer Engineering, University of Zagreb, Zagreb, Croatia; Faculty of Electrical and Computer Engineering, University of Zagreb, Zagreb, Croatia; Faculty of Electrical and Computer Engineering, University of Zagreb, Zagreb, Croatia; Faculty of Electrical and Computer Engineering, University of Zagreb, Zagreb, Croatia",IEEE Robotics and Automation Letters,"18 Mar 2021","2021","6","2","2768","2774","This letter presents a compliant exploration framework based on a collaborative robot Franka Panda that builds a 3D plant stem model. The model is built for agricultural plant treatment procedures without external sensors, as contact forces are estimated from joint torques and robot's dynamic and kinematic model. By devising an impedance-based exploratory control algorithm capable of following an unknown shape, while being provided with only a general direction in which to explore, we eliminate the need for a precise position controller. Our approach is validated through experiments with several mock-up plant stems, showing that the proposed framework is capable of building a satisfactory 3D model of a plant. The method is evaluated against the ground truth model, and compared to the state of the art approach based on an industrial manipulator with external sensors.","2377-3766","","10.1109/LRA.2021.3062301","Faculty of Electrical and Computer Engineering; Sveučilište u Zagrebu; Hrvatska Zaklada za Znanost(grant numbers:UIP-2017-05-4042); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363547","Compliance and impedance control;force and tactile sensing;robotics and automation in agriculture and forestry","Robots;Robot sensing systems;Force;End effectors;Three-dimensional displays;Collaboration;Torque","","16","","29","IEEE","25 Feb 2021","","","IEEE","IEEE Journals"
"AI-Augmented Robotic System for Agriculture: Intelligent Packing Application","A. Stepanova; H. Pham; A. Zourmand","Häme University of Applied Sciences, Hämeenlinna, Finland; Häme University of Applied Sciences, Hämeenlinna, Finland; Häme University of Applied Sciences, Hämeenlinna, Finland","2024 IEEE 12th Conference on Systems, Process & Control (ICSPC)","5 Feb 2025","2024","","","275","279","This paper introduces an improved robotic solution for the agriculture and food industry, applying Artificial Intelligence (AI) in tomato picking and packing processes. The work proposes a cost-effective integration of Internet of Things (IoT) and Machine-to-Machine (M2M) communication during post-harvest processing of tomatoes, facilitated by a computer vision-based robotic arm. The system leverages You Only Look Once (YOLO) model for accurate tomato identification, localization and relative size estimation. Our IoT implementation enables users to send commands specifying the desired tomato quantities and sizes for packing. The system also offers seamless control and monitoring capabilities for the maintenance personnel. Command transmission occurs through a local network incorporating a collaborative robot UR5 and Zivid Two industrial 3D camera, interconnected through a laptop and a Raspberry Pi microcomputer. The proposed system has achieved picking success rate of 0.98 and packing success rate of 0.95. This work addresses concerns related to food safety and labor shortages in small and medium enterprises (SMEs) by contributing to the automation of tomato processing and ensuring consistent quality of fresh, perishable tomatoes.","2769-7916","979-8-3503-9139-8","10.1109/ICSPC63060.2024.10862972","European Social Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10862972","Agriculture;Artificial Intelligence;Internet of Things;Machine Vision;Robotics","YOLO;Technological innovation;Three-dimensional displays;Service robots;Containers;Manipulators;Agriculture;Internet of Things;Artificial intelligence;Monitoring","","","","9","IEEE","5 Feb 2025","","","IEEE","IEEE Conferences"
"Hierarchy in Algorithm-Based Feedback to Patients Working With a Robotic Rehabilitation System: Toward User-Experience Optimization","D. Fruchter; R. Feingold Polak; S. Berman; S. Levy-Tzedek","Department of Industrial and Management Engineering, Ben-Gurion University of the Negev, Be'er Sheva, Israel; Recanati School for Community Health Professions, Department of Physical Therapy, Ben-Gurion University of the Negev, Be'er Sheva, Israel; Department of Industrial Engineering and Management and the Zlotowski Center for Neuroscience, Ben-Gurion University of the Negev, Be'er Sheva, Israel; Department of Physical Therapy and the Zlotowski Center for Neuroscience, Ben-Gurion University of the Negev, Be'er Sheva, Israel",IEEE Transactions on Human-Machine Systems,"15 Sep 2022","2022","52","5","907","917","When robotic systems are developed for individualized training during rehabilitation, providing effective feedback to the users is essential. We aimed to create a rule-based set of guidelines for the desired hierarchy, timing, content, and modality of feedback such a system should provide to users, such that they receive the necessary feedback, at the relevant time, in a way that enhances their performance, and does not encumber it. We conducted four focus groups with 20 stroke clinicians. The clinicians described the guiding principles they use when giving feedback to patients, and noted different output should be provided to the patient versus the clinician by the rehabilitation system. They delineated a hierarchy for providing feedback during the exercise set: success on the task is the primary goal, and feedback should be given on this aspect first. Once success is achieved, feedback should be given on the quality of the movement. Only when the task is successfully completed, with no compensatory movements (i.e., high quality), feedback should be given on movement speed. Using a follow-up survey and the member-checking approach, it was revealed that this hierarchical structure applies to early stages in the rehabilitation process, and that quality of movement becomes paramount as the rehabilitation process progresses. The clinicians expressed their desire to receive a full report on the patient's performance in each exercise session, which is more comprehensive than the feedback provided to the patient in real time. We conclude with a set of guidelines for developing automated feedback for patient populations.","2168-2305","","10.1109/THMS.2022.3170831","Leona M. and Harry B. Helmsley Charitable Trust; Agricultural, Biological and Cognitive Robotics Initiative; Marcus Endowment Fund; Paul Ivanier Center for Production Management; Ben-Gurion University of the Negev; Borten Family Foundation; Consolidated Anti-Aging Foundation; Ministry of Health, State of Israel; National Insurance Institute of Israel; European Union's Horizon 2020 research and innovation programme; Marie Skłodowska-Curie(grant numbers:754340); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774360","Artificial intelligence;exergames;participatory design;patient-centered;socially assistive robotics;user experience","Robots;Stroke (medical condition);Task analysis;Visualization;Interviews;Robot sensing systems;Guidelines","","8","","55","CCBYNCND","13 May 2022","","","IEEE","IEEE Journals"
"Containerized Vertical Farming Using Cobots","D. Mahalingam; A. Patankar; K. Phi; N. Chakraborty; R. McGann; I. Ramakrishnan","Department of Mechanical Engineering, Stony Brook University, USA; Department of Mechanical Engineering, Stony Brook University, USA; Department of Computer Science, Stony Brook University, USA; Department of Mechanical Engineering, Stony Brook University, USA; CubicAcres LLC, Stony Brook, USA; Department of Computer Science, Stony Brook University, USA",2024 IEEE International Conference on Robotics and Automation (ICRA),"8 Aug 2024","2024","","","17897","17903","Containerized vertical farming is a type of vertical farming practice using hydroponics in which plants are grown in vertical layers within a mobile shipping container. Space limitations within shipping containers make the automation of different farming operations challenging. In this paper, we explore the use of cobots (i.e., collaborative robots) to automate two key farming operations, namely, the transplantation of saplings and the harvesting of grown plants. Our method uses a single demonstration from a farmer to extract the motion constraints associated with the tasks, namely, transplanting and harvesting, and can then generalize to different instances of the same task. For transplantation, the motion constraint arises during insertion of the sapling within the growing tube, whereas for harvesting, it arises during extraction from the growing tube. We present experimental results to show that using RGBD camera images (obtained from an eye-in-hand configuration) and one demonstration for each task, it is feasible to perform transplantation of saplings and harvesting of leafy greens using a cobot, without task-specific programming.","","979-8-3503-8457-4","10.1109/ICRA57147.2024.10609985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609985","","Fingers;Robot vision systems;Collaborative robots;Containers;Programming;Cameras;Electron tubes","","","","48","IEEE","8 Aug 2024","","","IEEE","IEEE Conferences"
"Machine Learning Based Hardware Accelerated Cobot Grasping in the Food Industry","N. Ivačko; I. Ćirić; Ž. Cojbašić; M. Milošević; D. Jevtić","Faculty of Mechanical Engineering, University of Nis, Nis, Serbia; Faculty of Mechanical Engineering, University of Nis, Nis, Serbia; Faculty of Mechanical Engineering, University of Nis, Nis, Serbia; Faculty of Mechanical Engineering, University of Nis, Nis, Serbia; Fazi doo, Belgrade, Serbia","2024 11th International Conference on Electrical, Electronic and Computing Engineering (IcETRAN)","3 Sep 2024","2024","","","1","5","In food industry automation, the integration of machine learning with robotic systems can introduce improved efficiency and precision in various tasks such as fruit and vegetable handling and manipulation. This paper introduces an innovative approach to robotic grasping, specifically customized for the challenge of identifying and handling apples and oranges within a dynamic and unstructured environment. Utilizing the YOLO (You Only Look Once) object detection algorithm, the system consists of a camera mounted on the robot's end effector, enabling real-time identification and localization of fruits and vegetables. Implemented on the NVIDIA Jetson platform, this solution presents the combination of machine learning techniques with hardware acceleration, ensuring optimal performance in real-time object recognition and manipulation tasks. Our experimental setup tests the robustness of our object recognition and localization approach under varying lighting and orientation scenarios and evaluates the system's overall performance in a real-world, cluttered environment typical of the food industry. The findings of this study underscore the potential of integrating ML-powered vision systems with hardware-accelerated robotics for food handling and manipulation applications. By demonstrating high levels of accuracy in recognition, classification and grasping, our research contributes to the broader field of robotic automation, offering insights into the scalability and adaptability of such systems across different sectors of the food industry.","","979-8-3503-8699-8","10.1109/IcETRAN62308.2024.10645149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645149","Robot vision;Collaborative robots;Object recognition;Grasping;Image processing;Deep learning","YOLO;Service robots;Robot vision systems;Food industry;Machine learning;Grasping;Real-time systems","","2","","24","IEEE","3 Sep 2024","","","IEEE","IEEE Conferences"
"Design and prototyping of a low-cost light weight fixed-endpoint orientation planar Cobot","I. S. Howard","School of Engineering, Computing & Mathematics, University of Plymouth, Plymouth, UK",2022 International Conference on System Science and Engineering (ICSSE),"21 Nov 2022","2022","","","047","054","Here we present the design and construction of a low-cost planar robotic arm that makes use of light weight component and a passive link mechanism to maintain fixed endpoint orientation. The arm structure itself is low-cost and built from carbon fiber tubes which yields a high stiffness to weight ratio. To facilitate construction, commercially available pulley and bearing components are used in the design where possible and all custom mechanical parts are 3D printed. To reduce power consumption, the arm makes use of non-back-drivable worm-gear motor actuation, so static arm configurations can be maintained without requiring motor power. We first analyze and simulate the kinematics and the static torque/force relationships of the mechanism. A microcontroller system was then developed to read the sensors and drive the arm motors. Finally, we demonstrate arm operation with simple movement tasks.","2325-0925","978-1-6654-8852-5","10.1109/ICSSE55923.2022.9947353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9947353","Worm-drive;3D printing;Robot arm design;Agricultural robotics;Low-cost Cobot manipulator","Optical fiber sensors;Three-dimensional displays;Power demand;Microcontrollers;Pulleys;Manipulators;Robot sensing systems","","2","","20","IEEE","21 Nov 2022","","","IEEE","IEEE Conferences"
"Multi Agricultural Robots Cooperation Using Wireless Ad-hoc Network","P. Lin; X. Li; Z. Dong; L. Zhang","Engineering Research Center for Intelligent Robotics, Ji Hua Laboratory, Foshan, Guangdong, China; Engineering Research Center for Intelligent Robotics, Ji Hua Laboratory, Foshan, Guangdong, China; Engineering Research Center for Intelligent Robotics, Ji Hua Laboratory, Foshan, Guangdong, China; Engineering Research Center for Intelligent Robotics, Ji Hua Laboratory, Foshan, Guangdong, China",2022 IEEE 17th Conference on Industrial Electronics and Applications (ICIEA),"12 Jan 2023","2022","","","371","375","Nowadays, the compound robot is a kind of AGV mobile collaborative robot with the multi-axis robot mounted on it, which can be used for agricultural works, such as harvesting and inspecting. Meanwhile, often WLAN is used to provide wireless communication for prototype system, for research testbeds, and also for commercial and industrial applications. People manually assign tasks to robots in advance and supervise the job situation. Once the number of working robots changes, it is depressing to reassign the tasks manually again and again with lower automation level. The use of the optimized WLAN which the paper proposed makes greater automation and overcomes the above defects. A brief presentation of the behavior of an improved ad-hoc routing protocols proposed in this paper is given and important aspects are discussed exemplarily.","2158-2297","978-1-6654-0984-1","10.1109/ICIEA54703.2022.10005991","Jihua Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005991","Ad-hoc;Wireless networks;Routing protocol;Agricultural works;Collaborative robots","Wireless communication;Wireless LAN;Automation;Service robots;Routing;Routing protocols;Ad hoc networks","","","","11","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Robotics Solution for Agriculture: Automated and IoT-enabled Tomato Picking and Packing","A. Stepanova; H. Pham; A. Zourmand","HAMK Tech Research Unit Häme Universitv of Applied Sciences, Hämeenlinna, Finland; HAMK Tech Research Unit Häme Universitv of Applied Sciences, Hämeenlinna, Finland; HAMK Tech Research Unit Häme Universitv of Applied Sciences, Hämeenlinna, Finland","2023 IEEE International Conference on Agrosystem Engineering, Technology & Applications (AGRETA)","28 Sep 2023","2023","","","108","112","This paper presents a novel robotics solution for the food industry, focusing on automated and IoT-enabled tomato picking and packing. The work includes a pilot execution of a low-cost implementation of Internet of Things (IoT) and Machine to Machine (M2M) communication for the postharvest stage, specifically targeting tomato picking and primary packing using a computer vision-based robotic arm. The vision system employs You Only Look Once (YOLO) model weights to accurately detect and localize tomatoes. The proposed IoT development enables users to send commands specifying the desired number of tomatoes to be packed, while also providing easy control and monitoring capabilities for workers. The command transmission occurs via a local network that incorporates a collaborative robot UR5, a Zivid Two 3D industrial camera connected to a laptop, and a Raspberry Pi microcomputer. This work addresses the crucial aspects of food safety and labor shortage in small and medium-sized enterprises (SMEs), while also paving the way for automated tomato processing to ensure the high quality of fresh, perishable tomatoes.","","979-8-3503-4733-3","10.1109/AGRETA57740.2023.10262632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262632","Computer Vision;Food Industry;IoT;Robotics","Three-dimensional displays;Service robots;Machine vision;Robot vision systems;Containers;Safety;Internet of Things","","2","","13","IEEE","28 Sep 2023","","","IEEE","IEEE Conferences"
"Indoor and Outdoor Face Recognition for Social Robot, Sanbot Robot as Case Study","E. Ashtari; M. A. Basiri; S. M. Nejati; H. Zandi; S. H. S. Rezaei; M. T. Masouleh; A. Kalhor","School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran; School of Electrical and Computer Engineering, University of Tehran, Tehran, Iran",2020 28th Iranian Conference on Electrical Engineering (ICEE),"26 Nov 2020","2020","","","1","7","The interaction between human and robots is of paramount importance in comforting robot and human in the context of social demand. For the purpose of human-robot interaction, the robot should have the ability to perform a variety of actions including face recognition, path planning, etc. In this paper, face recognition has been implemented on the Sanbot robot. Since the Sanbot robot is intended to work in real environment, therefore indoor and outdoor environment is taken into account in proposing the corresponding face recognition algorithm. For each case a robust pre-processing algorithm should be designed and which can circumvent a challenging problem in face recognition, namely, different lighting conditions (light intensity, angle of radiation, etc.). In case of indoor environment, faces in an captured image by the robot HD camera are found using a Haar-cascade algorithm. Afterwards, a histogram equalization is applied to face images in order to standardize them. Then commonly practiced Deep convolutional neural network structures such as Inception and ResNet are used to design a model and trained end-to-end on a customized dataset with strong augmentation. Finally, by using a voting method, proper prediction is carried out on each face. In what concerns the outdoor environment, which has more challenges, upon applying histogram Equalization on the captured image, faces are found using a MultiTask Cascaded Convolutional Neural Network. Then face images are aligned as head orientation are corrected. Finally, cropped face image is fed to Siamese Network in order to extract face features and verifying individuals. From several practical results it has been inferred that the accuracy of the indoor method is nearly 93% without voting and with voting 97%, and the outdoor method is about 95%.","2642-9527","978-1-7281-7296-5","10.1109/ICEE50131.2020.9260698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9260698","Face recognition;Deep Neural Network;Siamese Neural Network;Sanbot robot;Social robot;Human-robot interaction","Face recognition;Robots;Cameras;Servers;Training;Testing;Streaming media","","4","","18","IEEE","26 Nov 2020","","","IEEE","IEEE Conferences"
"Human Movement Imitation for Robot Therapy using the NAO Robot","C. D. S. Ompico; E. D. Subido; J. N. Banayo; F. E. T. Munsayac; R. G. Baldovino; N. T. Bugtai","Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University 2401 Taft Avenue, Manila, Philippines; Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University 2401 Taft Avenue, Manila, Philippines; Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University 2401 Taft Avenue, Manila, Philippines; Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University 2401 Taft Avenue, Manila, Philippines; Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University 2401 Taft Avenue, Manila, Philippines; Institute of Biomedical Engineering and Health Technologies (IBEHT), De La Salle University 2401 Taft Avenue, Manila, Philippines","2021 IEEE 13th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)","16 Mar 2022","2021","","","1","5","One of the methods currently used in healthcare, with the use of social robots, is called robot therapy. This type of therapy is most commonly used for accompanying elders, and those with mental disorders. The main point of robot therapy is to provide nonhuman companionship, as those with mental disorders, especially anxiety, prefer non-judgmental company. However, the robots used mostly are currently in animal forms, which imitate the behavior of animals. This study aims to approach a more human-like behavior, but as robots cannot possess any judgement, patients are likely to feel more comfortable expressing themselves. Human movement is focused upon before proceeding to behavior imitation. In this study, only the upper limbs are detected and imitated using the concept of imitation learning. The user’s joints are detected by a 3D camera that sends the data to a program that computes the change in angle for each joint, which then causes the humanoid to execute similar movements. This research made use of the robot operating system (ROS) making the system flexible and easy to improve upon.","","978-1-6654-0167-8","10.1109/HNICEM54116.2021.9731985","Tokyo University of Agriculture and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9731985","imitation learning;robot therapy;ROS;social robot","Three-dimensional displays;Animals;Delay effects;Robot vision systems;Medical treatment;Humanoid robots;Programming","","1","","13","IEEE","16 Mar 2022","","","IEEE","IEEE Conferences"
"Multi-Vision-Based Picking Point Localisation of Target Fruit for Harvesting Robots","C. Beldek; A. Dunn; J. Cunningham; E. Sariyildiz; S. L. Phung; G. Alici","School of Mechanical, Materials, Mechatronic, and Biomedical Engineering, University of Wollongong, Wollongong, NSW, Australia; School of Mechanical, Materials, Mechatronic, and Biomedical Engineering, University of Wollongong, Wollongong, NSW, Australia; School of Mechanical, Materials, Mechatronic, and Biomedical Engineering, University of Wollongong, Wollongong, NSW, Australia; School of Mechanical, Materials, Mechatronic, and Biomedical Engineering, University of Wollongong, Wollongong, NSW, Australia; School of Electrical, Computer and Telecommunications Engineering, University of Wollongong, Wollongong, NSW, Australia; School of Mechanical, Materials, Mechatronic, and Biomedical Engineering, University of Wollongong, Wollongong, NSW, Australia",2025 IEEE International Conference on Mechatronics (ICM),"26 Mar 2025","2025","","","1","6","This paper presents multi-vision-based localisation strategies for harvesting robots. Identifying picking points accurately is essential for robotic harvesting because insecure grasping can lead to economic loss through fruit damage and dropping. In this study, two multi-vision-based localisation methods, namely the analytical approach and model-based algorithms, were employed. The actual geometric centre points of fruits were collected using a motion capture system (mocap), and two different surface points Cfix and Ceih were extracted using two Red-Green-Blue-Depth (RGB-D) cameras. First, the picking points of the target fruit were detected using analytical methods. Second, various primary and ensemble learning methods were employed to predict the geometric centre of target fruits by taking surface points as input. Adaboost regression, the most successful model-based localisation algorithm, achieved 88.8% harvesting accuracy with a Mean Euclidean Distance (MED) of 4.40 mm, while the analytical approach reached 81.4% picking success with a MED of 14.25 mm, both demonstrating better performance than the single-camera, which had a picking success rate of 77.7% with a MED of 24.02 mm. To evaluate the effect of picking point accuracy in collecting fruits, a series of robotic harvesting experiments were performed utilising a collaborative robot (cobot). It is shown that multi-vision systems can improve picking point localisation, resulting in higher success rates of picking in robotic harvesting.","2837-1151","979-8-3315-3389-2","10.1109/ICM62621.2025.10934868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10934868","Agriculture;Harvesting robots;Multi-vision-based Localisation;Ensemble Learning;Adaboost Regression","Analytical models;Accuracy;Mechatronics;Shape;Crops;Collaborative robots;Grasping;Prediction algorithms;Motion capture;Ensemble learning","","","","34","IEEE","26 Mar 2025","","","IEEE","IEEE Conferences"
"Dishflipper: Development of a Large Food Debris Rinsing and Removal System Integration for Fully Automating Dish-Washing in a Pilot Soba Noodle Stand","W. S. Lo; S. Ozeki; K. Nakashima; K. Tsukamoto; T. Sawanobori; I. Mizuuchi",Tokyo University of Agriculture and Technology; Tokyo University of Agriculture and Technology; Connected Robotics; Connected Robotics; Connected Robotics; Tokyo University of Agriculture and Technology,2024 IEEE/SICE International Symposium on System Integration (SII),"9 Feb 2024","2024","","","301","307","This work presents a system integration designed for washing reusable dishes at a soba noodle restaurant. To address the need to manually rinse food debris and flip the dishes when in our previous system integration, we developed an add-on system that utilizes the Dishflipper mechanism mounted on an robot-arm, improving the overall capability of our system. Our proposed system integration is unique as it considers automating the removal of large food pieces initially left in the tableware using a cobot. The developed system integration has been shown effective in matching the workflow of a soba noodle stand, as demonstrated at the Hotels and Restaurant Show 2021 (HCJ) hosted in Tokyo. (Extended video: https://www.youtube.com/watch?v=-DcNNwO5h1I)","2474-2325","979-8-3503-1207-2","10.1109/SII58957.2024.10417567","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417567","","Shape;Lips;System integration;Grasping;Cleaning;Complexity theory;Robots","","1","","15","IEEE","9 Feb 2024","","","IEEE","IEEE Conferences"
"A generic and modularized Digital twin enabled human-robot collaboration","X. Lu; X. Li; W. Wang; K. -M. Chao; L. Xu; P. De Vrieze; Y. Jing","Department of Computing and Informatics, Bournemouth University, Bournemouth, UK; College of Informatics, Huazhong Agricultural University, Wuhan, China; School of Engineering Science, University of Skövde, Skövde, Sweden; Department of Computing and Informatics, Bournemouth University, Bournemouth, UK; Department of Computing and Informatics, Bournemouth University, Bournemouth, UK; Department of Computing and Informatics, Bournemouth University, Bournemouth, UK; Faculty of Business, Computing and Digital industries, Leeds Trinity University, Leeds, UK",2022 IEEE International Conference on e-Business Engineering (ICEBE),"7 Feb 2023","2022","","","66","73","Recently, the manufacturing paradigm shifts from mass production to mass customization, which results in urgently demands for the development of intelligent, flexible and automatic manufacturing systems for handling complex manufacturing tasks with high efficiency. The use of collaborative robots, an essential enabling technology for developing human-robot collaboration (HRC), is on the rise for human-centric intelligent automation design. An effective virtual simulation platform, which can continuously simulate and evaluate HRC performance in different working scenarios, is lacking in developing an HRC system in a sophisticated industrial arena. This paper presents a generic and modularized digital twin enabled HRC framework based on the synergy effect of human, robotic and environment-related factors to provide a flexible, compatible, re-configurable solution to ease the implementation of HRC in the real world. The feasibility of the proposed framework is validated through the practical implementation of a food packaging job, which involves a human operator and an ABB robotic arm collaboratively working together, on an industrial shop","","978-1-6654-9244-7","10.1109/ICEBE55470.2022.00021","Knowledge Foundation(grant numbers:EU FoF-06-2014); Natural Science Foundation of China(grant numbers:61803169); Fundamental Research Funds for the Central Universities(grant numbers:2662018JC029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035060","Human-robot collaboration;Digital Twin;collaborative robot","Intelligent automation;Industries;Mass production;Mass customization;Service robots;Collaboration;Manipulators","","5","","18","IEEE","7 Feb 2023","","","IEEE","IEEE Conferences"
"Adaptive Model Verification for Modularized Industry 4.0 Applications","X. Xin; S. L. Keoh; M. Sevegnani; M. Saerbeck; T. P. Khoo","School of Computing Science, University of Glasgow, Glasgow, U.K.; School of Computing Science, University of Glasgow, Glasgow, U.K.; School of Computing Science, University of Glasgow, Glasgow, U.K.; Digital Service, TÜV SÜD Asia Pacific, International Business Park, Singapore; Digital Service, TÜV SÜD Asia Pacific, International Business Park, Singapore",IEEE Access,"6 Dec 2022","2022","10","","125353","125364","Cyber-Physical Systems (CPSs) are the core of Industry 4.0 applications, integrating advanced technologies such as sensing, data analytics, and artificial intelligence. This kind of combination typically consists of networked sensors and decision-making processes in which sensor-generated data drive the control decisions. Hence, the trustworthiness of the sensors is essential to guarantee performance, safety and quality during operation. Formal model verification techniques are a valuable tool allowing strong reasoning about the high-level design of CPSs. However, the uncertainty exhibited by the underlying sensor networks is often ignored. Manufacturing processes typically involve composition of various modular CPSs that work as a whole, such as multiple Collaborative Robots (cobots) working together as a production line, which improves the flexibility and resilience of the production process. It is still challenging to verify this class of compositional process while also considering uncertainty. We propose a novel verification framework for modular CPSs that combines sensor-level data-driven fault detection and system-level model-driven probabilistic model checking. The resulting framework can rigorously quantify sensor readings’ trustworthiness, enabling formal reasoning for system failure prediction and reliability analysis. We validated our approach on a cobots-based manufacturing process.","2169-3536","","10.1109/ACCESS.2022.3225399","Singapore Economic Development Board (EDB); Industrial Postgraduate Programme (IPP) Grant; Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/F033206/1); Formal Methods for Agritech Resilience Modelling (FARM)(grant numbers:EP/S035362/1); Multi-Perspective Design of IoT Cybersecurity in Ground and Aerial Vehicles (MAGIC); Amazon Research Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965400","Probabilistic model checking;sensor networks;trustworthiness;cyber-physical system;collaborative robot;industry 4.0","Sensors;Probabilistic logic;Adaptation models;Model checking;Sensor systems;Robot sensing systems;Monitoring","","2","","42","CCBY","28 Nov 2022","","","IEEE","IEEE Journals"
"Impression Change on Nonverbal Non-Humanoid Robot by Interaction with Humanoid Robot","A. Ueno; K. Hayashi; I. Mizuuchi","Department of Mechanical Systems Engineering, Tokyo University of Agriculture and Technology, Koganei, Japan; Department of Computer Science and Engineering, Toyohashi University of Technology, Toyohashi, Japan; Department of Mechanical Systems Engineering, Tokyo University of Agriculture and Technology, Koganei, Japan",2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),"13 Jan 2020","2019","","","1","6","Even if a robot is not designed with a specific impression, if there is a means that can add an impression later to the robot, it will be useful for social robot design, we considered. In particular, anthropomorphism seems to be an important impression of designing social interaction between humans and robots. In the movie, ”STAR WARS,” there is a non-humanoid robot, called R2-D2, which communicates mainly by sounds. A humanoid interpreter robot, called C-3PO, responds to the sound of R2-D2 with natural language and gesture. And the audience finds the personality in R2-D2 richer than the personality which is based on the information which R2-D2’s sounds have. It might be possible to change the impression of a non-humanoid robot emitting simple sounds by communication with a humanoid robot that speaks a natural language and make gestures. We conducted an impression evaluation experiment. In the condition where robots are interacting, the observer evaluated anthropomorphism of the nonhumanoid robot more than in the non-interacting condition. There were also some other impressions that have changed.","1944-9437","978-1-7281-2622-7","10.1109/RO-MAN46459.2019.8956240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8956240","","Natural languages;Social robots;Humanoid robots;Stars;Observers;Motion pictures;Anthropomorphism;Robots","","6","","18","IEEE","13 Jan 2020","","","IEEE","IEEE Conferences"
"Social Robots that can Sense and Improve Student Engagement","M. -L. Bourguet; Y. Jin; Y. Shi; Y. Chen; L. Rincon-Ardila; G. Venture","School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK; International School, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China; International School, Beijing University of Posts and Telecommunications, Beijing, China; Department of Mechanical Systems Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan; Department of Mechanical Systems Engineering, Tokyo University of Agriculture and Technology, Tokyo, Japan","2020 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)","8 Mar 2021","2020","","","127","134","It is highly likely that classrooms of the future will feature robots to assist the human teachers. Tutor robots will be valued for their capacity to motivate learners and to provide affective support during learning activities, which will require from them to be able to understand the students' affects and behaviours, and to respond to these through appropriate expressive motions. In this paper, we investigate the impact a robot teacher's behaviour has on the students' level of engagement. We outline research work we have carried out to tackle four of the challenges inherent to the effective deployment of social robots in classrooms: (1) sensing and understanding learners' affective states and behaviours in class; (2) combining affect and behaviour understanding to capture classroom's dynamics; (3) knowing what gestures a social robot should use as a learning facilitator; and (4) equipping the tutor robot with expressive and motivational capabilities.","2470-6698","978-1-7281-6942-2","10.1109/TALE48869.2020.9368438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9368438","social robots;emotional support;motivational pedagogical gestures;affect and behaviour understanding","Measurement;Conferences;Education;Robot sensing systems;Cognition;Sensors","","13","","30","IEEE","8 Mar 2021","","","IEEE","IEEE Conferences"
"Vision-Based Complete Scene Understanding Using Faster Region-Convolutional Neural Network","T. M R; S. Kumar Sahoo; M. B M; T. Soujanya; S. Kandaneri Ramamoorthy","Department of Computer Science and Engineering, Malnad College of Engineering, Hassan, India; VIT Bhopal University, Kothrikalan, India; Department of ECE, Nitte Meenakshi Institute of Technology, Bengaluru, India; School of Agricultural Sciences, SR University, Warangal, India; College of Non-Medicine, University: Texila American University, Guyana, South America",2024 International Conference on Data Science and Network Security (ICDSNS),"1 Oct 2024","2024","","","1","5","The computer vision is enabling the computer to process the obj ect identification and face recognition using artificial intelligence and machine learning. In computer vision the Human activity recognition (HAR) systems plays a key factor to capture from static images or playing videos. It can capture basic activities like walking and running but complex activities is a tough challenge to capture its environment objects for recognizes. In this proposed paper work address the faster Region-Convolutional Neural Network (R-CNN) method for holistic video understanding task, to enhance the human activity recognition. The semantic level description of the scene is the key factor for holistic video environment. It recognizes the human activities with autonomous system in robot and provides the contextual knowledge of the action event. Then the vision module and the social robot are integrated for natural and realistic context-based human robot interaction. The social robots need to familiar with the surrounding of the environment to react correctly for different scenarios. The faster R-CNN method obtain the accuracy with 89% with HAR dataset. The importance of contextual understanding in human activity is highlighted in recognition system.","","979-8-3503-7311-0","10.1109/ICDSNS62112.2024.10690903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10690903","computer vision;fast R-CNN method;human activity recognition (HAR);holistic video understanding;human robot interaction","Computer vision;Visualization;Accuracy;Social robots;Neural networks;Semantics;Network security;Robustness;Human activity recognition;Videos","","","","15","IEEE","1 Oct 2024","","","IEEE","IEEE Conferences"
"Thinking Aloud with a Tutoring Robot to Enhance Learning","A. Ramachandran; C. -M. Huang; E. Gartland; B. Scassellati",Yale University; Johns Hopkins University; Greens Farms Academy; Yale University,2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI),"8 Jul 2021","2018","","","59","68","Thinking aloud, while requiring extra mental effort, is a metacognitive technique that helps students navigate through complex problem-solving tasks. Social robots, bearing embodied immediacy that fosters engaging and compliant interactions, are a unique platform to deliver problem-solving support such as thinking aloud to young learners. In this work, we explore the effects of a robot platform and the think-aloud strategy on learning outcomes in the context of a one-on-one tutoring interaction. Results from a 2x2 between-subjects study (n = 52) indicate that both the robot platform and use of the think-aloud strategy promoted learning gains for children. In particular, the robot platform effectively enhanced immediate learning gains, measured right after the tutoring session, while the think-aloud strategy improved persistent gains as measured approximately one week after the interaction. Moreover, our results show that a social robot strengthened students’ engagement and compliance with the think-aloud support while they performed cognitively demanding tasks. Our work indicates that robots can support metacognitive strategy use to effectively enhance learning and contributes to the growing body of research demonstrating the value of social robots in novel educational settings.","2167-2148","978-1-4503-4953-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9473626","Child-robot Interaction;Education;Tutoring","Atmospheric measurements;Navigation;Human-robot interaction;Gain measurement;Particle measurements;Problem-solving;Task analysis","","6","","43","","8 Jul 2021","","","IEEE","IEEE Conferences"
"An Online Toolkit for Applications Featuring Collaborative Robots Across Different Domains","J. Saenz; J. Bessler-Etten; M. Valori; G. B. Prange-Lasonder; I. Fassi; C. Bidard; A. B. Lassen; I. Paniti; A. Toth; T. Stuke; S. Wrede; K. Nielsen","Business Unit Robotic Systems, Fraunhofer IFF, Magdeburg, Germany; Roessingh Research and Development, Enschede, The Netherlands; Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing of the National Research Council of Italy (CNR STIIMA), Bari, Italy; Roessingh Research and Development, Enschede, The Netherlands; Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing of the National Research Council of Italy (CNR STIIMA), Milan, Italy; Université Paris-Saclay, Palaiseau, France; Department of Robot Technology, Danish Technological Institute, Odense, Denmark; Centre of Excellence in Production Informatics and Control, ELKH SZTAKI, Budapest, Hungary; DARPAMotion Ltd., Budapest, Hungary; Weidmüller Interface GmbH & Company KG, Detmold, Germany; Research Institute for Cognition and Robotics, University of Bielefeld, Bielefeld, Germany; Department of Robot Technology, Danish Technological Institute, Odense, Denmark",IEEE Transactions on Human-Machine Systems,"31 Jul 2023","2023","53","4","657","667","Collaborative robots (cobots) are being applied in areas such as healthcare, rehabilitation, agriculture and logistics, beyond the typical manufacturing setting. This is leading to a marked increase in the number of cobot stakeholders with little or no experience in traditional safety engineering. Considering the importance of human safety in collaborative robotic applications, this is currently proving to be a barrier to more widespread cobot usage. A web-based Toolkit that targets cobot end-users and manufacturers with varying levels of safety expertise was developed, helping them to understand how to consider the safety of their cobot applications. In this work, we will provide an overview of the state of the art for ensuring cobot safety, highlight the support provided by the “COVR Toolkit” and introduce three examples where third parties applied the Toolkit for their collaborative robotics application.","2168-2305","","10.1109/THMS.2022.3213416","European Union's Horizon 2020 research and innovation programme(grant numbers:779966); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931662","Human-robot interaction;intelligent robots;occupational safety;robots;safety;standardization","Robots;Safety;Stakeholders;Standards;Service robots;Risk management;Medical services","","6","","43","CCBY","27 Oct 2022","","","IEEE","IEEE Journals"
"COVR Toolkit – Supporting safety of interactive robotics applications","J. Saenz; I. Fassi; G. B. Prange-Lasonder; M. Valori; C. Bidard; A. B. Lassen; J. Bessler-Etten","Business Unit Robotic Systems Fraunhofer IFF, Magdeburg, Germany; Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing National Research Council of Italy, Milan, Italy; Roessingh Research and Development, Enschede, The Netherlands; Institute of Intelligent Industrial Technologies and Systems for Advanced Manufacturing National Research Council of Italy, Milan, Italy; Université Paris-Saclay CEA List, Palaiseau, France; Department of Robot Technology, Danish Technological Institute, Odense, Denmark; Roessingh Research and Development, University of Twente, Enschede, The Netherlands",2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS),"27 Oct 2021","2021","","","1","6","Collaborative robots (cobots) are increasingly finding use beyond the traditional domain of manufacturing, in areas such as healthcare, rehabilitation, agriculture and logistics. This development greatly increases the size and variations in the level of expertise of cobot stakeholders. This becomes particularly critical considering the role of human safety for collaborative robotics applications. In order to support the wide range of cobot stakeholders, the EU-funded project COVR “Being safe around collaborative and versatile robots in shared spaces” has developed a freely available, web-based Toolkit that offers support to understand how to consider the safety of cobot applications. This paper describes the state of the art for ensuring safety across various life cycle phases in the development and implementation of collaborative robotics applications and highlights how the Toolkit provides practical support during these tasks. The Toolkit aims to be the most comprehensive resource for supporting cobot stakeholders in ensuring the safety of their applications.","","978-1-6654-0170-8","10.1109/ICHMS53169.2021.9582659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582659","physical human-robot interaction;safety;collaborative robotics;standardization","Collaboration;Medical services;Regulation;Safety;Manufacturing;Stakeholders;Risk analysis","","5","","16","IEEE","27 Oct 2021","","","IEEE","IEEE Conferences"
"Analysis and Improvement in Healthcare Operation Utilizing Automation","Y. Amer; L. T. T. Doan; W. A. P. Dania; T. T. Tran","UniSA STEM, The University of South Australia, Adelaide, South Australia, Australia; UniSA STEM, The University of South Australia, Adelaide, South Australia, Australia; Faculty of Agricultural Techonlogy, University of Brawijaya, Malang, Indonesia; Department of Industrial Management, Can Tho University, Can Tho City, Vietnam","2022 International Conference on Control, Robotics and Informatics (ICCRI)","2 Sep 2022","2022","","","88","95","The development of various medications, vaccines and surgical methods in healthcare has been significantly contributing to longer life expectancy. However, there are still various challenges that need to be addressed in the healthcare system. One of the biggest challenges in streamlining processes is that medical staff are required to do various repetitive tasks manually. This study aims to examine how automation and robotics can be utilized to improve the efficiency of healthcare/biomedical services. The high-end collaborative robot, the “YuMi” robot is proposed to modify and streamline biomedical lab operations to work side-by-side with biomedical lab technicians to assist various repetitive and routine tasks. Four potential frameworks (i.e., aged care, sorting and dispensing medicines, assisting at surgical operations and sample collection) are developed to identify potential applications of the Yumi robot. With the Covid-19 pandemic situation, there is an immediate need for safe sample collections and patient interaction to mitigate the outbreak of the virus. In this study, a framework for the sample collection and testing of Covid-19 is also proposed to minimize the risks of medical staff and local transmissions. Samples testing of suspected patients, travellers, and those in close contact with Covid-19 patients will proceed without direct interactions with healthcare workers, eventually, minimizing exposure and spread of this communicable disease.","","978-1-6654-6800-8","10.1109/ICCRI55461.2022.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870728","automation;healthcare services/biomedical;Yumi robot;Covid-19;robot applications","COVID-19;Automation;Pandemics;Infectious diseases;Vaccines;Task analysis;Informatics","","2","","25","IEEE","2 Sep 2022","","","IEEE","IEEE Conferences"
"Evaluating Guided Policy Search for Human-Robot Handovers","A. Kshirsagar; G. Hoffman; A. Biess","Sibley School of Mechanical and Aerospace Engineering, Cornell University, Ithaca, NY, USA; Sibley School of Mechanical and Aerospace Engineering, Cornell University, Ithaca, NY, USA; Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Be’er Sheva, Israel",IEEE Robotics and Automation Letters,"7 Apr 2021","2021","6","2","3933","3940","We evaluate the potential of Guided Policy Search (GPS), a model-based reinforcement learning (RL) method, to train a robot controller for human-robot object handovers. Handovers are a key competency for collaborative robots and GPS could be a promising approach for this task, as it is data efficient and does not require prior knowledge of the robot and environment dynamics. However, existing uses of GPS did not consider important aspects of human-robot handovers, namely large spatial variations in reach locations, moving targets, and generalizing over mass changes induced by the object being handed over. In this work, we formulate the reach phase of handovers as an RL problem and then train a collaborative robot arm in a simulation environment. Our results indicate that GPS is limited in the spatial generalizability over variations in the target location, but that this issue can be mitigated with the addition of local controllers trained over target locations in the high error regions. Moreover, learned policies generalize well over a large range of end-effector masses. Moving targets can be reached with comparable errors using a global policy trained on static targets, but this results in inefficient, high-torque, trajectories. Training on moving targets improves trajectories, but results in worse worst-case performance. Initial results suggest that lower-dimensional state representations are beneficial for GPS performance in handovers.","2377-3766","","10.1109/LRA.2021.3067299","Mills Family Faculty Fellowship; Helmsley Charitable Trust through the Agricultural, Biological, and Cognitive (ABC) Robotics Initiative; Ben-Gurion University of the Negev; Israel Science Foundation(grant numbers:1627/17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381633","Manipulation planning;physical human-robot interaction;reinforcement learning","Global Positioning System;Robots;Handover;Task analysis;Training;Trajectory;Heuristic algorithms","","10","","36","CCBY","18 Mar 2021","","","IEEE","IEEE Journals"
"Water Based Magnification of Capacitive Proximity Sensors: Water Containers as Passive Human Detectors","R. P. Rocha; A. T. de Almeida; M. Tavakoli","Institute of Systems and Robotics, University of Coimbra, Portugal; Institute of Systems and Robotics, University of Coimbra, Portugal; Institute of Systems and Robotics, University of Coimbra, Portugal",2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"10 Feb 2021","2020","","","11058","11064","Sensors that detect human presence received an increasing attention due to the recent advances in smart homes, collaborative fabrication cells, and human robot interaction. These sensors can be used in collaborative robot cells and mobile robots, in order to increase the robot awareness about the presence of humans, in order to increase safety during their operation. Among proximity detection systems, capacitive sensors are interesting, since they are low cost and simple human proximity detectors, however their detection range is limited. In this article, we show that the proximity detection range of a capacitive sensor can be enhanced, when the sensor is placed near a water container. In addition, the signal can pass trough several adjacent water containers, even if they are separated by a few centimeters. This phenomenon has an important implication in establishing low cost sensor networks. For instance, a limited number of active capacitive sensor nodes can be linked with several simple passive nodes, i.e. water containers, to detect human or animal proximity in a large area such as a farm, a factory or home. Analysis on the change of the maximum proximity range with sensor dimension, container size and liquid filler was performed in order to study this effect. Examples of application are also demonstrated.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9340877","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340877","","Liquids;Detectors;Containers;Sensor phenomena and characterization;Robot sensing systems;Capacitive sensors;Intelligent sensors","","","","21","IEEE","10 Feb 2021","","","IEEE","IEEE Conferences"
"Non-Destructive Testing of Composite Fiber Materials With Hyperspectral Imaging—Evaluative Studies in the EU H2020 FibreEUse Project","Y. Yan; J. Ren; H. Zhao; J. F. C. Windmill; W. Ijomah; J. de Wit; J. von Freeden","National Subsea Centre, Robert Gordon University, Aberdeen, U.K.; National Subsea Centre, Robert Gordon University, Aberdeen, U.K.; Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, U.K.; Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, U.K.; Department of Design, Manufacturing and Engineering Management, University of Strathclyde, Glasgow, U.K.; INVENT GmbH, Braunschweig, Germany; Fraunhofer Institute for Machine Tools and Forming Technology IWU, Wolfsburg, Germany",IEEE Transactions on Instrumentation and Measurement,"22 Mar 2022","2022","71","","1","13","Through capturing spectral data from a wide frequency range along with the spatial information, hyperspectral imaging (HSI) can detect minor differences in terms of temperature, moisture, and chemical composition. Therefore, HSI has been successfully applied in various applications, including remote sensing for security and defense, precision agriculture for vegetation and crop monitoring, food/drink, and pharmaceuticals quality control. However, for condition monitoring and damage detection in carbon fiber reinforced polymer (CFRP), the use of HSI is a relatively untouched area, as existing non-destructive testing (NDT) techniques focus mainly on delivering information about physical integrity of structures but not on material composition. To this end, HSI can provide a unique way to tackle this challenge. In this article, with the use of a near-infrared (NIR) HSI camera, applications of HSI for the non-destructive inspection of CFRP products are introduced, taking the European Union (EU) H2020 FibreEUse project as the background. Technical challenges and solutions on three case studies are presented in detail, including adhesive residues detection, surface damage detection, and cobot-based automated inspection. Experimental results have fully demonstrated the great potential of HSI and related vision techniques for NDT of CFRP, especially the potential to satisfy the industrial manufacturing environment.","1557-9662","","10.1109/TIM.2022.3155745","European Union (EU) H2020 (FibreEUse)(grant numbers:730323); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9726887","Carbon fiber reinforced polymer (CFRP);H2020;hyperspectral imaging (HSI);non-destructive inspection","Inspection;Testing;Hyperspectral imaging;Imaging;Cameras;Service robots;Polymers","","22","","38","IEEE","3 Mar 2022","","","IEEE","IEEE Journals"
"A Novel Approach for Leveraging Object Detection for 3D Human Pose Estimation in Complex Human-Robot Collaboration Environments","S. Süme; A. K. Ajay; T. M. Wendt; S. J. Rupitsch","Work-Life Robotics Institute, University of Applied Sciences Offenburg, Offenburg, Germany; Work-Life Robotics Institute, University of Applied Sciences Offenburg, Offenburg, Germany; Work-Life Robotics Institute, University of Applied Sciences Offenburg, Offenburg, Germany; Department of Microsystems Engineering, Laboratory for Electrical Instrumentation and Embedded Systems, University of Freiburg, Freiburg, Germany",2025 IEEE 21st International Conference on Automation Science and Engineering (CASE),"23 Sep 2025","2025","","","1134","1139","Although 3D Human Pose Estimation has major breakthroughs in recent years, 3D pose estimation in complex scenarios remains difficult. One of the reasons is the lack of diverse 3D datasets for training and generalizing the models. This issue is counteracted by acquiring a dataset of Human-Robot Collaboration scenes featuring different objects, such as a cobot. We propose a novel two-step method, where first a 3D Object detection task with VoteNet is performed to identify the human in the scenario and claim it as a region of interest for the pose estimation task. Second, this region of interest is cropped and passed into the 3D Human Pose Estimation algorithm SPiKE, which locates 15 keypoints of the human. Based on this procedure, our method improves detection in complex scenarios. Furthermore, this article compares the benefits of training the algorithm additionally on the obtained Human-Robot Collaboration dataset compared to training it with the standard ITOP dataset. While the SPiKE algorithm makes no correct prediction on the Human-Robot Collaboration scenario, the results of the two-step SPiKEVN approach with mAP of 41.17 % is significantly lower as the benchmark model on the ITOP dataset. Nonetheless, the SPiKEVN model exhibits similar performance to SPiKEman with a difference of 2.43 % mAP indicating the method is effectively functioning.","2161-8089","979-8-3315-2246-9","10.1109/CASE58245.2025.11164071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11164071","","Training;Three-dimensional displays;Working environment noise;Pose estimation;Pipelines;Human-robot interaction;Object detection;Prediction algorithms;Standards;Synthetic data","","","","22","IEEE","23 Sep 2025","","","IEEE","IEEE Conferences"
